datasets:
  # Pre-training Datasets
  - name: "the-stack-v2"
    type: "huggingface"
    path: "HuggingFaceH4/the-stack-v2"
    stage: "pretraining"
    split: "train"
    max_seq_length: 8192
    num_training_steps: 100000
  - name: "github-code"
    type: "huggingface"
    path: "codeparrot/github-code"
    stage: "pretraining"
    split: "train"
    max_seq_length: 8192
    num_training_steps: 100000

  # Supervised Fine-Tuning (SFT) Datasets
  - name: "magicoder"
    type: "huggingface"
    path: "ise-uiuc/Magicoder-Evol-Instruct-110K"
    stage: "sft"
    split: "train"
    max_seq_length: 4096
    num_training_steps: 50000
  - name: "code-feedback"
    type: "huggingface"
    path: "TIGER-Lab/CodeFeedback"
    stage: "sft"
    split: "train"
    max_seq_length: 4096
    num_training_steps: 50000

  # RLHF / GRPO Datasets
  - name: "code-ultrafeedback"
    type: "huggingface"
    path: "HuggingFaceH4/code-ultrafeedback"
    stage: "rlhf"
    split: "train_prefs"
    max_seq_length: 2048
    num_training_steps: 25000

  # Evaluation Datasets
  - name: "swe-bench"
    type: "huggingface"
    path: "princeton-nlp/swe-bench"
    stage: "eval"
    split: "test"
    max_seq_length: 4096
    num_training_steps: 0
  - name: "codeforces"
    type: "huggingface"
    path: "BAAI/CodeF"
    stage: "eval"
    split: "test"
    max_seq_length: 4096
    num_training_steps: 0
